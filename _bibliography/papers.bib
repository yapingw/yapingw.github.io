@article{FASQR,
	abbr = {Job Market Paper},
	author = {Yaping Wang},
	note = {Draft and slides coming soon},
	title = {Bridging Dense and Sparse Models in High-Dimensional Quantile Regression},
	year = {2025}, journal = {}, 
    volume = {},
    number = {},
    pages = {}}

@article{ERM-PCR,
	abbr = {Working Paper},
	abstract = {This paper establishes bounds on the predictive performance of empirical risk minimization for principal component regression. Our analysis is nonparametric, in the sense that the relation between the prediction target and the predictors is not specified. In particular, we do not rely on the assumption that the prediction target is generated by a factor model. In our analysis we consider the cases in which the largest eigenvalues of the covariance matrix of the predictors grow linearly in the number of predictors (strong signal regime) or sublinearly (weak signal regime). The main result of this paper shows that empirical risk minimization for principal component regression is consistent for prediction and, under appropriate conditions, it achieves near-optimal performance in both the strong and weak signal regimes.},
	author = {Yaping Wang and Christian Brownlees and Guðmundur Stef{\'a}n Guðmundsson},
	note = {R&R at Econometric Theory},  arxiv = {2409.03606},
	pdf = {https://yapingw.github.io/assets/pdf/erm_pcr.pdf},
	slides = {https://yapingw.github.io/assets/pdf/erm_pcr_slides.pdf},
	title = {Performance of Empirical Risk Minimization For Principal Component Regression},
	year = {2025}}

@article{C-V,
	abbr = {Working Paper},
	abstract = {This paper studies how to determine the number of factors $k$ by cross-validation in high-dimensional predictive models.
We consider a nonparametric setting in which the relationship between the target variable and high-dimensional predictors  is unspecified, and treat the number of factors as a tuning parameter for prediction: factors are estimated fold-wise from predictors only, $k$ is selected to minimize the validation loss in predicting the target, and factors are re-estimated on the full sample once is $k$ chosen.
We show that fold-wise cross-validation achieves near-oracle out-of-sample predictive performance under both strong and weak factors regimes.
Extensions to weakly dependent data are derived using blocked cross-validation, providing valid performance guarantees for factor-augmented predictions with time series.
Our empirical application shows that cross-validated factor selection yields smaller out-of-sample prediction errors than information-criterion-based choices, particularly when the factors are weak or their directions are misaligned with the target.
},
	author = {Yaping Wang },
	title = {Cross-Validating the Number of Factors for Prediction},
	year = {2025}}
